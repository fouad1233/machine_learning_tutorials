{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.describe of       Unnamed: 0 label                                               text  \\\n",
      "0            605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
      "1           2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
      "2           3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
      "3           4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
      "4           2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
      "...          ...   ...                                                ...   \n",
      "5166        1518   ham  Subject: put the 10 on the ft\\r\\nthe transport...   \n",
      "5167         404   ham  Subject: 3 / 4 / 2000 and following noms\\r\\nhp...   \n",
      "5168        2933   ham  Subject: calpine daily gas nomination\\r\\n>\\r\\n...   \n",
      "5169        1409   ham  Subject: industrial worksheets for august 2000...   \n",
      "5170        4807  spam  Subject: important online banking alert\\r\\ndea...   \n",
      "\n",
      "      label_num  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             1  \n",
      "4             0  \n",
      "...         ...  \n",
      "5166          0  \n",
      "5167          0  \n",
      "5168          0  \n",
      "5169          0  \n",
      "5170          1  \n",
      "\n",
      "[5171 rows x 4 columns]>\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('spam.csv', encoding='ISO-8859-1')\n",
    "print(df.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.describe of      label                                               text\n",
      "0      ham  Subject: enron methanol ; meter # : 988291\\r\\n...\n",
      "1      ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...\n",
      "2      ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...\n",
      "3     spam  Subject: photoshop , windows , office . cheap ...\n",
      "4      ham  Subject: re : indian springs\\r\\nthis deal is t...\n",
      "...    ...                                                ...\n",
      "5166   ham  Subject: put the 10 on the ft\\r\\nthe transport...\n",
      "5167   ham  Subject: 3 / 4 / 2000 and following noms\\r\\nhp...\n",
      "5168   ham  Subject: calpine daily gas nomination\\r\\n>\\r\\n...\n",
      "5169   ham  Subject: industrial worksheets for august 2000...\n",
      "5170  spam  Subject: important online banking alert\\r\\ndea...\n",
      "\n",
      "[5171 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "df = df.drop([\"Unnamed: 0\", \"label_num\"], axis=1)\n",
    "print(df.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.describe of      label                                               text\n",
      "0      ham  Subject: enron methanol ; meter # : 988291\\r\\n...\n",
      "1      ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...\n",
      "2      ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...\n",
      "3     spam  Subject: photoshop , windows , office . cheap ...\n",
      "4      ham  Subject: re : indian springs\\r\\nthis deal is t...\n",
      "...    ...                                                ...\n",
      "5166   ham  Subject: put the 10 on the ft\\r\\nthe transport...\n",
      "5167   ham  Subject: 3 / 4 / 2000 and following noms\\r\\nhp...\n",
      "5168   ham  Subject: calpine daily gas nomination\\r\\n>\\r\\n...\n",
      "5169   ham  Subject: industrial worksheets for august 2000...\n",
      "5170  spam  Subject: important online banking alert\\r\\ndea...\n",
      "\n",
      "[5171 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "df.columns = ['label', 'text']\n",
    "print(df.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text  b_labels\n",
      "0   ham  Subject: enron methanol ; meter # : 988291\\r\\n...         0\n",
      "1   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...         0\n",
      "2   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...         0\n",
      "3  spam  Subject: photoshop , windows , office . cheap ...         1\n",
      "4   ham  Subject: re : indian springs\\r\\nthis deal is t...         0\n"
     ]
    }
   ],
   "source": [
    "df['b_labels'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['b_labels'].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2739    Subject: free pay per view\\r\\nhad chance at la...\n",
      "4748                                        Subject: \\r\\n\n",
      "1884    Subject: april , aspect volume @ texas city\\r\\...\n",
      "3933                      Subject: you can be smart !\\r\\n\n",
      "87      Subject: enron nom for march 7 , 2001\\r\\n( see...\n",
      "                              ...                        \n",
      "2276    Subject: re : fw : cowtrap allocation - 4 / 01...\n",
      "1196    Subject: spot purchases - 04 / 01\\r\\nthe follo...\n",
      "2454    Subject: re : coastal ctr # 96008903 meter 098...\n",
      "1957    Subject: physical curve mappings _ sitara\\r\\nh...\n",
      "2268    Subject: a new era of online medical care .\\r\\...\n",
      "Name: text, Length: 3464, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(decode_error='ignore')\n",
    "x_train = tfidf.fit_transform(x_train)\n",
    "x_test = tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 305683 stored elements and shape (3464, 41009)>\n",
      "  Coords\tValues\n",
      "  (0, 35439)\t0.004465463541488112\n",
      "  (0, 17345)\t0.01631367655544015\n",
      "  (0, 28464)\t0.01902340510969144\n",
      "  (0, 28651)\t0.01622309609587272\n",
      "  (0, 38716)\t0.06708019133621743\n",
      "  (0, 18933)\t0.06562453007790081\n",
      "  (0, 9666)\t0.04500824696558316\n",
      "  (0, 6052)\t0.01876578071725469\n",
      "  (0, 23136)\t0.032812265038950406\n",
      "  (0, 36932)\t0.02436948981976569\n",
      "  (0, 36524)\t0.220330350309614\n",
      "  (0, 39210)\t0.06734838905330129\n",
      "  (0, 8371)\t0.07553171476647629\n",
      "  (0, 23380)\t0.06934126783912486\n",
      "  (0, 10623)\t0.11283787981509283\n",
      "  (0, 4852)\t0.06855721271546586\n",
      "  (0, 20666)\t0.04983926264146977\n",
      "  (0, 26970)\t0.041050565909519415\n",
      "  (0, 6515)\t0.049235144943267854\n",
      "  (0, 12638)\t0.024029226048788637\n",
      "  (0, 36659)\t0.045305972104200475\n",
      "  (0, 40457)\t0.20204516715990387\n",
      "  (0, 14021)\t0.2157316063538136\n",
      "  (0, 12424)\t0.07553171476647629\n",
      "  (0, 21483)\t0.07852233384648227\n",
      "  :\t:\n",
      "  (3463, 15816)\t0.10623660485317449\n",
      "  (3463, 5009)\t0.1083245725336835\n",
      "  (3463, 18592)\t0.0992012952165496\n",
      "  (3463, 27585)\t0.1853809201506768\n",
      "  (3463, 36612)\t0.11186261085453435\n",
      "  (3463, 24461)\t0.11889445580433491\n",
      "  (3463, 16577)\t0.10598847467623929\n",
      "  (3463, 39915)\t0.12593447302334201\n",
      "  (3463, 15499)\t0.1571713589337329\n",
      "  (3463, 37935)\t0.1319562098249444\n",
      "  (3463, 37478)\t0.15498381705008527\n",
      "  (3463, 36617)\t0.14126237112025744\n",
      "  (3463, 19157)\t0.14638392125863586\n",
      "  (3463, 24990)\t0.2507067666135639\n",
      "  (3463, 27691)\t0.18480472340374932\n",
      "  (3463, 26322)\t0.17820189688558682\n",
      "  (3463, 25459)\t0.18480472340374932\n",
      "  (3463, 34777)\t0.1730803467472084\n",
      "  (3463, 27084)\t0.19411088469906232\n",
      "  (3463, 27216)\t0.19411088469906232\n",
      "  (3463, 25978)\t0.19411088469906232\n",
      "  (3463, 29793)\t0.19411088469906232\n",
      "  (3463, 15202)\t0.19411088469906232\n",
      "  (3463, 38765)\t0.19411088469906232\n",
      "  (3463, 25008)\t0.19411088469906232\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9174364896073903\n",
      "test score: 0.8681898066783831\n"
     ]
    }
   ],
   "source": [
    "#classify using naive bayes\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "print(\"train score:\", model.score(x_train, y_train))\n",
    "print(\"test score:\", model.score(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def fit(self, X, y):\n",
    "        # Separate documents by class\n",
    "        self.spam_docs = X[y == 1]\n",
    "        self.ham_docs = X[y == 0]\n",
    "        \n",
    "        # Calculate the prior probabilities P(spam) and P(ham)\n",
    "        self.p_spam = len(self.spam_docs) / len(X)\n",
    "        self.p_ham = len(self.ham_docs) / len(X)\n",
    "        \n",
    "        # Calculate word counts for spam and ham\n",
    "        self.spam_word_count = np.sum(self.spam_docs, axis=0)\n",
    "        self.ham_word_count = np.sum(self.ham_docs, axis=0)\n",
    "        print(self.spam_word_count)\n",
    "        # Total word counts for spam and ham documents\n",
    "        self.spam_total = np.sum(self.spam_word_count)\n",
    "        self.ham_total = np.sum(self.ham_word_count)\n",
    "        \n",
    "        # Vocabulary size\n",
    "        self.vocab_size = X.shape[1]\n",
    "        \n",
    "        # Calculate conditional probabilities with Laplace smoothing\n",
    "        self.spam_prob = (self.spam_word_count + 1) / (self.spam_total + self.vocab_size)\n",
    "        self.ham_prob = (self.ham_word_count + 1) / (self.ham_total + self.vocab_size)\n",
    "    \n",
    "    def predict_log_proba(self, X):\n",
    "        # Calculate log probabilities for the given X based on learned probabilities\n",
    "        log_prob_spam = X @ np.log(self.spam_prob) + np.log(self.p_spam)\n",
    "        log_prob_ham = X @ np.log(self.ham_prob) + np.log(self.p_ham)\n",
    "        \n",
    "        # Combine into a matrix of log probabilities for each class\n",
    "        return np.vstack([log_prob_ham, log_prob_spam]).T\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Get the log probabilities for ham and spam\n",
    "        log_probs = self.predict_log_proba(X)\n",
    "        \n",
    "        # Choose the class with the higher probability (log space)\n",
    "        return np.argmax(log_probs, axis=1)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        # Predict and check the accuracy\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "    def precision(self, X, y):\n",
    "        # Predict and check the precision\n",
    "        predictions = self.predict(X)\n",
    "        return np.sum(predictions[y == 1] == 1) / np.sum(predictions == 1)\n",
    "    def recall(self, X, y):\n",
    "        # Predict and check the recall\n",
    "        predictions = self.predict(X)\n",
    "        return np.sum(predictions[y == 1] == 1) / np.sum(y == 1)\n",
    "    def f1_score(self, X, y):\n",
    "        # Calculate the F1 score\n",
    "        precision = self.precision(X, y)\n",
    "        recall = self.recall(X, y)\n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def fit(self, X, y):\n",
    "        # Separate documents by class\n",
    "        self.C = [X[y == i] for i in np.unique(y)]\n",
    "        \n",
    "        # Calculate the prior probabilities P(spam) and P(ham)\n",
    "        self.p_C = [len(self.C[i]) / len(X) for i in range(len(np.unique(y)))]\n",
    "        \n",
    "        # Calculate word counts for spam and ham\n",
    "        self.C_count = [np.sum(self.C[i], axis=0) for i in range(len(np.unique(y)))]\n",
    "        \n",
    "        self.c_total = [np.sum(self.C_count[i]) for i in range(len(np.unique(y)))]\n",
    "        \n",
    "        # Vocabulary size\n",
    "        self.vocab_size = X.shape[1]\n",
    "        \n",
    "        # Calculate conditional probabilities with Laplace smoothing\n",
    "        self.C_prob = [(self.C_count[i] + 1) / (self.c_total[i] + self.vocab_size) for i in range(len(np.unique(y)))]\n",
    "    \n",
    "    def predict_log_proba(self, X):\n",
    "        \n",
    "        log_prob_C = [X @ np.log(self.C_prob[i]) + np.log(self.p_C[i]) for i in range(len(np.unique(y)))]\n",
    "        \n",
    "        # Combine into a matrix of log probabilities for each class\n",
    "        return np.vstack(log_prob_C).T\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Get the log probabilities for ham and spam\n",
    "        log_probs = self.predict_log_proba(X)\n",
    "        \n",
    "        # Choose the class with the higher probability (log space)\n",
    "        return np.argmax(log_probs, axis=1)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        # Predict and check the accuracy\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "    def precision(self, X, y):\n",
    "        # Predict and check the precision\n",
    "        predictions = self.predict(X)\n",
    "        return np.sum(predictions[y == 1] == 1) / np.sum(predictions == 1)\n",
    "    def recall(self, X, y):\n",
    "        # Predict and check the recall\n",
    "        predictions = self.predict(X)\n",
    "        return np.sum(predictions[y == 1] == 1) / np.sum(y == 1)\n",
    "    def f1_score(self, X, y):\n",
    "        # Calculate the F1 score\n",
    "        precision = self.precision(X, y)\n",
    "        recall = self.recall(X, y)\n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform xtrain to numpy array\n",
    "x_train = x_train.toarray()\n",
    "x_test = x_test.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.64352803 3.42151291 0.46605495 ... 0.         0.11328037 0.0241156 ]\n",
      "train accuracy: 0.9174364896073903\n",
      "train precision: 1.0\n",
      "train recall: 0.717391304347826\n",
      "train f1 score: 0.8354430379746834\n",
      "test accuracy: 0.8681898066783831\n",
      "test precision: 1.0\n",
      "test recall: 0.5379876796714579\n",
      "test f1 score: 0.6995994659546061\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Naive Bayes Classifier\n",
    "nb = NaiveBayesClassifier()\n",
    "nb.fit(x_train, y_train)\n",
    "\n",
    "# Calculate training and test accuracy\n",
    "print(\"train accuracy:\", nb.accuracy(x_train, y_train))\n",
    "print(\"train precision:\", nb.precision(x_train, y_train))\n",
    "print(\"train recall:\", nb.recall(x_train, y_train))\n",
    "print(\"train f1 score:\", nb.f1_score(x_train, y_train))\n",
    "print(\"test accuracy:\", nb.accuracy(x_test, y_test))\n",
    "print(\"test precision:\", nb.precision(x_test, y_test))\n",
    "print(\"test recall:\", nb.recall(x_test, y_test))\n",
    "print(\"test f1 score:\", nb.f1_score(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
