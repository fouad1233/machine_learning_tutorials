{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spam.csv', encoding='ISO-8859-1')\n",
    "print(df.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"Unnamed: 0\", \"label_num\"], axis=1)\n",
    "print(df.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['label', 'text']\n",
    "print(df.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['b_labels'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['b_labels'].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesSpamClassifier:\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.messages = x_train\n",
    "        self.labels = y_train\n",
    "        \n",
    "        self.word_dict = {}\n",
    "\n",
    "        for i in range(len(self.messages)):\n",
    "            try:\n",
    "                for word in self.messages[i].split():\n",
    "                    #print(word)\n",
    "                    \n",
    "                    if word not in self.word_dict:\n",
    "                        word = str(word)\n",
    "                        self.word_dict[word] = (0, 0)\n",
    "                    #print(word)\n",
    "                    self.word_dict[word] = (self.word_dict[word][0] + (self.labels[i] == 0), self.word_dict[word][1] + (self.labels[i] == 1))\n",
    "            except:\n",
    "                    \n",
    "                    pass\n",
    "        self.spam_count = sum(self.labels)\n",
    "        self.ham_count = len(self.labels) - self.spam_count\n",
    "        self.p_spam = self.spam_count / len(self.labels)\n",
    "        self.p_ham = 1 - self.p_spam\n",
    "    def predict(self, message):\n",
    "        p_spam_message = self.p_spam\n",
    "        p_ham_message = self.p_ham\n",
    "        for word in message.split():\n",
    "            if word not in self.word_dict:\n",
    "                continue\n",
    "            p_spam_message *= (self.word_dict[word][1] + 1) / (self.spam_count + 1)\n",
    "            p_ham_message *= (self.word_dict[word][0] + 1) / (self.ham_count + 1)\n",
    "        return p_spam_message > p_ham_message\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        # Predict and check the accuracy\n",
    "        for message in X:\n",
    "            prediction = self.predict(message)\n",
    "\n",
    "        return np.mean(prediction == y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make x_train a list\n",
    "x_train = x_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train the Naive Bayes Classifier\n",
    "nb = NaiveBayesSpamClassifier()\n",
    "nb.fit(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training and test accuracy\n",
    "print(\"train accuracy:\", nb.accuracy(x_train, y_train))\n",
    "print(\"test accuracy:\", nb.accuracy(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(decode_error='ignore')\n",
    "x_train = tfidf.fit_transform(x_train)\n",
    "x_test = tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify using naive bayes\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "print(\"train accuracy:\", model.score(x_train, y_train))\n",
    "print(\"test score:\", model.score(x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def fit(self, X, y):\n",
    "        # Separate documents by class\n",
    "        self.spam_docs = X[y == 1]\n",
    "        self.ham_docs = X[y == 0]\n",
    "        \n",
    "        # Calculate the prior probabilities P(spam) and P(ham)\n",
    "        self.p_spam = len(self.spam_docs) / len(X)\n",
    "        self.p_ham = len(self.ham_docs) / len(X)\n",
    "        \n",
    "        # Calculate word counts for spam and ham\n",
    "        self.spam_word_count = np.sum(self.spam_docs, axis=0)\n",
    "        self.ham_word_count = np.sum(self.ham_docs, axis=0)\n",
    "        #print(self.spam_word_count)\n",
    "        # Total word counts for spam and ham documents\n",
    "        self.spam_total = np.sum(self.spam_word_count)\n",
    "        self.ham_total = np.sum(self.ham_word_count)\n",
    "        \n",
    "        # Vocabulary size\n",
    "        self.vocab_size = X.shape[1]\n",
    "        \n",
    "        # Calculate conditional probabilities with Laplace smoothing\n",
    "        self.spam_prob = (self.spam_word_count + 1) / (self.spam_total + self.vocab_size)\n",
    "        self.ham_prob = (self.ham_word_count + 1) / (self.ham_total + self.vocab_size)\n",
    "    \n",
    "    def predict_log_proba(self, X):\n",
    "        # Calculate log probabilities for the given X based on learned probabilities\n",
    "        log_prob_spam = X @ np.log(self.spam_prob) + np.log(self.p_spam)\n",
    "        log_prob_ham = X @ np.log(self.ham_prob) + np.log(self.p_ham)\n",
    "        \n",
    "        # Combine into a matrix of log probabilities for each class\n",
    "        return np.vstack([log_prob_ham, log_prob_spam]).T\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Get the log probabilities for ham and spam\n",
    "        log_probs = self.predict_log_proba(X)\n",
    "        \n",
    "        # Choose the class with the higher probability (log space)\n",
    "        return np.argmax(log_probs, axis=1)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        # Predict and check the accuracy\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "    def precision(self, X, y):\n",
    "        # Predict and check the precision\n",
    "        predictions = self.predict(X)\n",
    "        return np.sum(predictions[y == 1] == 1) / np.sum(predictions == 1)\n",
    "    def recall(self, X, y):\n",
    "        # Predict and check the recall\n",
    "        predictions = self.predict(X)\n",
    "        return np.sum(predictions[y == 1] == 1) / np.sum(y == 1)\n",
    "    def f1_score(self, X, y):\n",
    "        # Calculate the F1 score\n",
    "        precision = self.precision(X, y)\n",
    "        recall = self.recall(X, y)\n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def fit(self, X, y):\n",
    "        # Separate documents by class\n",
    "        self.C = [X[y == i] for i in np.unique(y)]\n",
    "        \n",
    "        # Calculate the prior probabilities P(spam) and P(ham)\n",
    "        self.p_C = [len(self.C[i]) / len(X) for i in range(len(np.unique(y)))]\n",
    "        \n",
    "        # Calculate word counts for spam and ham\n",
    "        self.C_count = [np.sum(self.C[i], axis=0) for i in range(len(np.unique(y)))]\n",
    "        \n",
    "        self.c_total = [np.sum(self.C_count[i]) for i in range(len(np.unique(y)))]\n",
    "        \n",
    "        # Vocabulary size\n",
    "        self.vocab_size = X.shape[1]\n",
    "        \n",
    "        # Calculate conditional probabilities with Laplace smoothing\n",
    "        self.C_prob = [(self.C_count[i] + 1) / (self.c_total[i] + self.vocab_size) for i in range(len(np.unique(y)))]\n",
    "    \n",
    "    def predict_log_proba(self, X):\n",
    "        \n",
    "        log_prob_C = [X @ np.log(self.C_prob[i]) + np.log(self.p_C[i]) for i in range(len(np.unique(y)))]\n",
    "        \n",
    "        # Combine into a matrix of log probabilities for each class\n",
    "        return np.vstack(log_prob_C).T\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Get the log probabilities for ham and spam\n",
    "        log_probs = self.predict_log_proba(X)\n",
    "        \n",
    "        # Choose the class with the higher probability (log space)\n",
    "        return np.argmax(log_probs, axis=1)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        # Predict and check the accuracy\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "    def precision(self, X, y):\n",
    "        # Predict and check the precision\n",
    "        predictions = self.predict(X)\n",
    "        return np.sum(predictions[y == 1] == 1) / np.sum(predictions == 1)\n",
    "    def recall(self, X, y):\n",
    "        # Predict and check the recall\n",
    "        predictions = self.predict(X)\n",
    "        return np.sum(predictions[y == 1] == 1) / np.sum(y == 1)\n",
    "    def f1_score(self, X, y):\n",
    "        # Calculate the F1 score\n",
    "        precision = self.precision(X, y)\n",
    "        recall = self.recall(X, y)\n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform xtrain to numpy array\n",
    "x_train = x_train.toarray()\n",
    "x_test = x_test.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train the Naive Bayes Classifier\n",
    "nb = NaiveBayesClassifier()\n",
    "nb.fit(x_train, y_train)\n",
    "\n",
    "# Calculate training and test accuracy\n",
    "print(\"train accuracy:\", nb.accuracy(x_train, y_train))\n",
    "print(\"train precision:\", nb.precision(x_train, y_train))\n",
    "print(\"train recall:\", nb.recall(x_train, y_train))\n",
    "print(\"train f1 score:\", nb.f1_score(x_train, y_train))\n",
    "print(\"test accuracy:\", nb.accuracy(x_test, y_test))\n",
    "print(\"test precision:\", nb.precision(x_test, y_test))\n",
    "print(\"test recall:\", nb.recall(x_test, y_test))\n",
    "print(\"test f1 score:\", nb.f1_score(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
